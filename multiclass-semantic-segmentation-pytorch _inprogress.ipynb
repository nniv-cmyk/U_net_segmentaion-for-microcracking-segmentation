{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinnr\\anaconda3\\envs\\pytorch\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/color_coded'\n",
    "\n",
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train_masks')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'val_masks')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'test_masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# class EggplantDataset(Dataset):\n",
    "#     def __init__(self, x_dir, y_dir, transform=None):\n",
    "#         self.x_dir = x_dir  # Directory containing the images\n",
    "#         self.y_dir = y_dir  # Directory containing the corresponding masks\n",
    "#         self.transform = transform\n",
    "\n",
    "#         # Get the list of filenames for images and masks\n",
    "#         self.image_filenames = [os.path.join(self.x_dir, filename) for filename in os.listdir(self.x_dir)]\n",
    "#         self.mask_filenames = [os.path.join(self.y_dir, filename) for filename in os.listdir(self.y_dir)]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_filenames)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # Load the image and mask at the given index\n",
    "#         image = Image.open(self.image_filenames[index])\n",
    "#         mask = Image.open(self.mask_filenames[index])\n",
    "\n",
    "#         # Apply transformations if provided\n",
    "#         if self.transform is not None:\n",
    "#             augmented = self.transform(image=image, mask=mask)\n",
    "#             image = augmented['image']\n",
    "#             mask = augmented['mask']\n",
    "\n",
    "#         # Convert the PIL images to tensors\n",
    "#         image = to_tensor(image)\n",
    "#         mask = to_tensor(mask)\n",
    "\n",
    "#         # As the mask contains multiple channels, reduce it to a single channel\n",
    "#         mask = torch.max(mask, dim=0)[0]\n",
    "\n",
    "#         return image, mask\n",
    "\n",
    "\n",
    "class EggplantDataset(Dataset):\n",
    "    def __init__(self, x_dir, y_dir, transform=None):\n",
    "        self.x_dir = x_dir  # Directory containing the images\n",
    "        self.y_dir = y_dir  # Directory containing the corresponding masks\n",
    "\n",
    "        # Get the list of filenames for images and masks\n",
    "        self.image_filenames = [os.path.join(self.x_dir, filename) for filename in os.listdir(self.x_dir)]\n",
    "        self.mask_filenames = [os.path.join(self.y_dir, filename) for filename in os.listdir(self.y_dir)]\n",
    "\n",
    "        # Define a dictionary to map color codes to class indices\n",
    "        self.color_to_class = {\n",
    "            (0, 0, 0): 0,    # Background\n",
    "            (0, 0, 255): 1,  # Calyx\n",
    "            (0, 255, 0): 2,  # Label\n",
    "            (255, 0, 0): 3,  # Fruit\n",
    "        }\n",
    "\n",
    "        # Add the Albumentations transform\n",
    "        self.transform = transform \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load the image and mask at the given index\n",
    "        image = Image.open(self.image_filenames[index])\n",
    "        mask = Image.open(self.mask_filenames[index])\n",
    "\n",
    "        # Convert the color-coded mask to class indices\n",
    "        mask = np.array(mask)\n",
    "        mask_class_indices = np.zeros(mask.shape[:2], dtype=np.uint8)\n",
    "        for color, class_index in self.color_to_class.items():\n",
    "            class_mask = np.all(mask == color, axis=-1)\n",
    "            mask_class_indices[class_mask] = class_index\n",
    "\n",
    "        # Convert PIL images to numpy arrays\n",
    "        image_np = np.array(image)\n",
    "        mask_np = mask_class_indices\n",
    "\n",
    "        # Apply the Albumentations transformation\n",
    "        augmented = self.transform(image=image_np, mask=mask_np)\n",
    "        image_np = augmented['image']\n",
    "        mask_np = augmented['mask']\n",
    "\n",
    "        # Convert numpy arrays to PyTorch tensors\n",
    "        image = torch.tensor(image_np, dtype=torch.float32).permute(2, 0, 1)\n",
    "        mask_class_indices = torch.tensor(mask_np, dtype=torch.long)\n",
    "\n",
    "        return image, mask_class_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 512)),  # Resize the images to (256, 512)\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create train, validation, and test datasets\n",
    "train_dataset = EggplantDataset(x_train_dir, y_train_dir, transform=transform)\n",
    "val_dataset = EggplantDataset(x_valid_dir, y_valid_dir, transform=transform)\n",
    "test_dataset = EggplantDataset(x_test_dir, y_test_dir, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create an instance of the EggplantDataset\n",
    "# dataset = EggplantDataset(x_train_dir, y_train_dir, transform=None)  # No transformations for visualization\n",
    "\n",
    "# # Define the number of random samples to visualize\n",
    "# num_samples_to_visualize = 5\n",
    "\n",
    "# # Get random indices for the samples\n",
    "# random_indices = random.sample(range(len(dataset)), num_samples_to_visualize)\n",
    "\n",
    "# # Plot the random images and masks\n",
    "# fig, axes = plt.subplots(num_samples_to_visualize, 2, figsize=(10, 5 * num_samples_to_visualize))\n",
    "\n",
    "# for i, idx in enumerate(random_indices):\n",
    "#     image, mask = dataset[idx]\n",
    "\n",
    "#     # Convert the mask to a NumPy array for visualization\n",
    "#     mask = mask.numpy()\n",
    "\n",
    "#     # Plot the image\n",
    "#     axes[i, 0].imshow(image.permute(1, 2, 0))  # Convert tensor back to (H, W, C) format\n",
    "#     axes[i, 0].axis('off')\n",
    "\n",
    "#     # Plot the mask\n",
    "#     axes[i, 1].imshow(mask, cmap='gray')  # Use a grayscale colormap for masks\n",
    "#     axes[i, 1].axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Create an instance of the EggplantDataset\n",
    "dataset = EggplantDataset(x_train_dir, y_train_dir, transform=None)  # No transformations for visualization\n",
    "\n",
    "# Define the number of random samples to visualize\n",
    "num_samples_to_visualize = 5\n",
    "\n",
    "# Get random indices for the samples\n",
    "random_indices = random.sample(range(len(dataset)), num_samples_to_visualize)\n",
    "\n",
    "# Plot the random images and masks\n",
    "# fig, axes = plt.subplots(num_samples_to_visualize, 2, figsize=(10, 5 * num_samples_to_visualize))\n",
    "\n",
    "# for i, idx in enumerate(random_indices):\n",
    "#     image, mask_class_indices = dataset[idx]\n",
    "    \n",
    "#     # Get the corresponding mask filename\n",
    "#     mask_filename = dataset.mask_filenames[idx]\n",
    "\n",
    "#     # Open the mask image as a PIL.Image object\n",
    "#     mask_image = Image.open(mask_filename)\n",
    "\n",
    "#     # Plot the image\n",
    "#     axes[i, 0].imshow(image.permute(1, 2, 0))  # Convert tensor back to (H, W, C) format\n",
    "#     axes[i, 0].axis('off')\n",
    "\n",
    "#     # Plot the mask\n",
    "#     axes[i, 1].imshow(mask_image)\n",
    "#     axes[i, 1].set_title(\"Mask\")\n",
    "#     axes[i, 1].axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LyftUdacity(Dataset):\n",
    "#     def __init__(self,img_dir,transform = None):\n",
    "#         self.transforms = transform\n",
    "#         image_paths = [i+'/CameraRGB' for i in img_dir]\n",
    "#         seg_paths = [i+'/CameraSeg' for i in img_dir]\n",
    "#         self.images,self.masks = [],[]\n",
    "#         for i in image_paths:\n",
    "#             imgs = os.listdir(i)\n",
    "#             self.images.extend([i+'/'+img for img in imgs])\n",
    "#         for i in seg_paths:\n",
    "#             masks = os.listdir(i)\n",
    "#             self.masks.extend([i+'/'+mask for mask in masks])\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "#     def __getitem__(self,index):\n",
    "#         img = np.array(Image.open(self.images[index]))\n",
    "#         mask = np.array(Image.open(self.masks[index]))\n",
    "#         if self.transforms is not None:\n",
    "#             aug = self.transforms(image=img,mask=mask)\n",
    "#             img = aug['image']\n",
    "#             mask = aug['mask']\n",
    "#             mask = torch.max(mask,dim=2)[0]\n",
    "#         return img,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = ['../input/lyft-udacity-challenge/data'+i+'/data'+i for i in ['A','B','C','D','E']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_images(image_dir,transform = None,batch_size=1,shuffle=True,pin_memory=True):\n",
    "#     data = LyftUdacity(image_dir,transform = t1)\n",
    "#     train_size = int(0.8 * data.__len__())\n",
    "#     test_size = data.__len__() - train_size\n",
    "#     train_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "#     train_batch = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n",
    "#     test_batch = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n",
    "#     return train_batch,test_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_images(x_train_dir, y_train_dir, transform=None, batch_size=1, shuffle=True, pin_memory=True):\n",
    "    # Create an instance of the EggplantDataset\n",
    "    dataset = EggplantDataset(x_train_dir, y_train_dir, transform=transform)\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create data loaders for train and test sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = A.Compose([\n",
    "#     A.Resize(160,240),\n",
    "#     A.augmentations.transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "# 375x1000\n",
    "\n",
    "t1 = A.Compose([\n",
    "    A.Resize(512, 256, interpolation=cv2.INTER_NEAREST),  # Resize with \"nearest\" interpolation\n",
    "    A.augmentations.transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_dir = r\".\\data\\color_coded\\train\"\n",
    "# y_train_dir = r\".\\data\\color_coded\\train_masks\"\n",
    "\n",
    "train_loader, test_loader = get_images(x_train_dir, y_train_dir, transform=t1, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch,test_batch = get_images(data_dir,transform =t1,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create train and test data loaders\n",
    "# train_loader, test_loader = get_images(x_train_dir, y_train_dir, transform=None, batch_size=1)\n",
    "\n",
    "# # Get a batch of data from the train_loader\n",
    "# for img_batch, mask_batch in train_loader:\n",
    "#     # Loop through each image and mask in the batch\n",
    "#     for i in range(len(img_batch)):\n",
    "#         img = img_batch[i]\n",
    "#         mask = mask_batch[i]\n",
    "\n",
    "#         # Convert the image tensor to a NumPy array and permute dimensions\n",
    "#         img_np = np.transpose(img.numpy(), (1, 2, 0))\n",
    "\n",
    "#         # Convert the mask tensor to a NumPy array\n",
    "#         mask_np = mask.numpy()\n",
    "\n",
    "#         # Plot the image and mask side by side\n",
    "#         fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "#         ax[0].imshow(img_np)\n",
    "#         ax[0].set_title(\"Image\")\n",
    "#         ax[0].axis(\"off\")\n",
    "#         ax[1].imshow(mask_np)\n",
    "#         ax[1].set_title(\"Mask\")\n",
    "#         ax[1].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     break  # Only visualize the first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchsummary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoding_block(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(encoding_block,self).__init__()\n",
    "        model = []\n",
    "        model.append(nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False))\n",
    "        model.append(nn.BatchNorm2d(out_channels))\n",
    "        model.append(nn.ReLU(inplace=True))\n",
    "        model.append(nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False))\n",
    "        model.append(nn.BatchNorm2d(out_channels))\n",
    "        model.append(nn.ReLU(inplace=True))\n",
    "        self.conv = nn.Sequential(*model)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unet_model(nn.Module):\n",
    "    def __init__(self,out_channels=23,features=[64, 128, 256, 512]):\n",
    "        super(unet_model,self).__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
    "        self.conv1 = encoding_block(3,features[0])\n",
    "        self.conv2 = encoding_block(features[0],features[1])\n",
    "        self.conv3 = encoding_block(features[1],features[2])\n",
    "        self.conv4 = encoding_block(features[2],features[3])\n",
    "        self.conv5 = encoding_block(features[3]*2,features[3])\n",
    "        self.conv6 = encoding_block(features[3],features[2])\n",
    "        self.conv7 = encoding_block(features[2],features[1])\n",
    "        self.conv8 = encoding_block(features[1],features[0])        \n",
    "        self.tconv1 = nn.ConvTranspose2d(features[-1]*2, features[-1], kernel_size=2, stride=2)\n",
    "        self.tconv2 = nn.ConvTranspose2d(features[-1], features[-2], kernel_size=2, stride=2)\n",
    "        self.tconv3 = nn.ConvTranspose2d(features[-2], features[-3], kernel_size=2, stride=2)\n",
    "        self.tconv4 = nn.ConvTranspose2d(features[-3], features[-4], kernel_size=2, stride=2)        \n",
    "        self.bottleneck = encoding_block(features[3],features[3]*2)\n",
    "        self.final_layer = nn.Conv2d(features[0],out_channels,kernel_size=1)\n",
    "    def forward(self,x):\n",
    "        skip_connections = []\n",
    "        x = self.conv1(x)\n",
    "        skip_connections.append(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        skip_connections.append(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        skip_connections.append(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv4(x)\n",
    "        skip_connections.append(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        x = self.tconv1(x)\n",
    "        x = torch.cat((skip_connections[0], x), dim=1)\n",
    "        x = self.conv5(x)\n",
    "        x = self.tconv2(x)\n",
    "        x = torch.cat((skip_connections[1], x), dim=1)\n",
    "        x = self.conv6(x)\n",
    "        x = self.tconv3(x)\n",
    "        x = torch.cat((skip_connections[2], x), dim=1)\n",
    "        x = self.conv7(x)        \n",
    "        x = self.tconv4(x)\n",
    "        x = torch.cat((skip_connections[3], x), dim=1)\n",
    "        x = self.conv8(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model(out_channels=4, features=[64, 128, 256, 512]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,728\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "              ReLU-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4         [-1, 64, 256, 256]          36,864\n",
      "       BatchNorm2d-5         [-1, 64, 256, 256]             128\n",
      "              ReLU-6         [-1, 64, 256, 256]               0\n",
      "    encoding_block-7         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-8         [-1, 64, 128, 128]               0\n",
      "            Conv2d-9        [-1, 128, 128, 128]          73,728\n",
      "      BatchNorm2d-10        [-1, 128, 128, 128]             256\n",
      "             ReLU-11        [-1, 128, 128, 128]               0\n",
      "           Conv2d-12        [-1, 128, 128, 128]         147,456\n",
      "      BatchNorm2d-13        [-1, 128, 128, 128]             256\n",
      "             ReLU-14        [-1, 128, 128, 128]               0\n",
      "   encoding_block-15        [-1, 128, 128, 128]               0\n",
      "        MaxPool2d-16          [-1, 128, 64, 64]               0\n",
      "           Conv2d-17          [-1, 256, 64, 64]         294,912\n",
      "      BatchNorm2d-18          [-1, 256, 64, 64]             512\n",
      "             ReLU-19          [-1, 256, 64, 64]               0\n",
      "           Conv2d-20          [-1, 256, 64, 64]         589,824\n",
      "      BatchNorm2d-21          [-1, 256, 64, 64]             512\n",
      "             ReLU-22          [-1, 256, 64, 64]               0\n",
      "   encoding_block-23          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-24          [-1, 256, 32, 32]               0\n",
      "           Conv2d-25          [-1, 512, 32, 32]       1,179,648\n",
      "      BatchNorm2d-26          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-27          [-1, 512, 32, 32]               0\n",
      "           Conv2d-28          [-1, 512, 32, 32]       2,359,296\n",
      "      BatchNorm2d-29          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-30          [-1, 512, 32, 32]               0\n",
      "   encoding_block-31          [-1, 512, 32, 32]               0\n",
      "        MaxPool2d-32          [-1, 512, 16, 16]               0\n",
      "           Conv2d-33         [-1, 1024, 16, 16]       4,718,592\n",
      "      BatchNorm2d-34         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-35         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-36         [-1, 1024, 16, 16]       9,437,184\n",
      "      BatchNorm2d-37         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-38         [-1, 1024, 16, 16]               0\n",
      "   encoding_block-39         [-1, 1024, 16, 16]               0\n",
      "  ConvTranspose2d-40          [-1, 512, 32, 32]       2,097,664\n",
      "           Conv2d-41          [-1, 512, 32, 32]       4,718,592\n",
      "      BatchNorm2d-42          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-43          [-1, 512, 32, 32]               0\n",
      "           Conv2d-44          [-1, 512, 32, 32]       2,359,296\n",
      "      BatchNorm2d-45          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-46          [-1, 512, 32, 32]               0\n",
      "   encoding_block-47          [-1, 512, 32, 32]               0\n",
      "  ConvTranspose2d-48          [-1, 256, 64, 64]         524,544\n",
      "           Conv2d-49          [-1, 256, 64, 64]       1,179,648\n",
      "      BatchNorm2d-50          [-1, 256, 64, 64]             512\n",
      "             ReLU-51          [-1, 256, 64, 64]               0\n",
      "           Conv2d-52          [-1, 256, 64, 64]         589,824\n",
      "      BatchNorm2d-53          [-1, 256, 64, 64]             512\n",
      "             ReLU-54          [-1, 256, 64, 64]               0\n",
      "   encoding_block-55          [-1, 256, 64, 64]               0\n",
      "  ConvTranspose2d-56        [-1, 128, 128, 128]         131,200\n",
      "           Conv2d-57        [-1, 128, 128, 128]         294,912\n",
      "      BatchNorm2d-58        [-1, 128, 128, 128]             256\n",
      "             ReLU-59        [-1, 128, 128, 128]               0\n",
      "           Conv2d-60        [-1, 128, 128, 128]         147,456\n",
      "      BatchNorm2d-61        [-1, 128, 128, 128]             256\n",
      "             ReLU-62        [-1, 128, 128, 128]               0\n",
      "   encoding_block-63        [-1, 128, 128, 128]               0\n",
      "  ConvTranspose2d-64         [-1, 64, 256, 256]          32,832\n",
      "           Conv2d-65         [-1, 64, 256, 256]          73,728\n",
      "      BatchNorm2d-66         [-1, 64, 256, 256]             128\n",
      "             ReLU-67         [-1, 64, 256, 256]               0\n",
      "           Conv2d-68         [-1, 64, 256, 256]          36,864\n",
      "      BatchNorm2d-69         [-1, 64, 256, 256]             128\n",
      "             ReLU-70         [-1, 64, 256, 256]               0\n",
      "   encoding_block-71         [-1, 64, 256, 256]               0\n",
      "           Conv2d-72          [-1, 4, 256, 256]             260\n",
      "================================================================\n",
      "Total params: 31,037,828\n",
      "Trainable params: 31,037,828\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 931.00\n",
      "Params size (MB): 118.40\n",
      "Estimated Total Size (MB): 1050.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinnr\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/133 [00:00<?, ?it/s]C:\\Users\\vinnr\\AppData\\Local\\Temp/ipykernel_14492/2527090450.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image_np, dtype=torch.float32).permute(2, 0, 1)\n",
      "C:\\Users\\vinnr\\AppData\\Local\\Temp/ipykernel_14492/2527090450.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask_class_indices = torch.tensor(mask_np, dtype=torch.long)\n",
      "C:\\Users\\vinnr\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "  0%|          | 0/133 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[1, 256, 3, 512] to have 3 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14492/2638600363.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14492/1413054190.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mskip_connections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mskip_connections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14492/3930682272.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 443\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[1, 256, 3, 512] to have 3 channels, but got 256 channels instead"
     ]
    }
   ],
   "source": [
    "# move the appropriate model to device\n",
    "model.to(DEVICE)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loop = tqdm(enumerate(train_loader),total=len(train_loader))\n",
    "    for batch_idx, (data, targets) in loop:\n",
    "        data = data.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        targets = targets.type(torch.long)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulation_steps = 4 # accumulate gradients over 4 steps\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "#     total_loss = 0.0\n",
    "\n",
    "# for batch_idx, (data, targets) in loop:\n",
    "#     data = data.to(DEVICE)\n",
    "#     targets = targets.to(DEVICE)\n",
    "#     targets = targets.type(torch.long)\n",
    "\n",
    "#     # clear gradients from previous iteration\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # Forward pass and loss calculation using autocast for mixed precision training\n",
    "#     with autocast():\n",
    "#         predictions = model(data)\n",
    "#         loss = loss_fn(predictions, targets)\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "\n",
    "#         # Perform optimization after accumulation streps\n",
    "#         if (batch_idx + 1) % accumulation_steps == 0:\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "        \n",
    "# #         # update tqdm loop\n",
    "#         loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            preds = torch.argmax(softmax(model(x)),axis=1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n",
    "\n",
    "    print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\")\n",
    "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(train_batch, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(test_batch, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in test_batch:\n",
    "    x = x.to(DEVICE)\n",
    "    fig , ax =  plt.subplots(3, 3, figsize=(18, 18))\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    preds = torch.argmax(softmax(model(x)),axis=1).to('cpu')\n",
    "    img1 = np.transpose(np.array(x[0,:,:,:].to('cpu')),(1,2,0))\n",
    "    preds1 = np.array(preds[0,:,:])\n",
    "    mask1 = np.array(y[0,:,:])\n",
    "    img2 = np.transpose(np.array(x[1,:,:,:].to('cpu')),(1,2,0))\n",
    "    preds2 = np.array(preds[1,:,:])\n",
    "    mask2 = np.array(y[1,:,:])\n",
    "    img3 = np.transpose(np.array(x[2,:,:,:].to('cpu')),(1,2,0))\n",
    "    preds3 = np.array(preds[2,:,:])\n",
    "    mask3 = np.array(y[2,:,:])\n",
    "    ax[0,0].set_title('Image')\n",
    "    ax[0,1].set_title('Prediction')\n",
    "    ax[0,2].set_title('Mask')\n",
    "    ax[1,0].set_title('Image')\n",
    "    ax[1,1].set_title('Prediction')\n",
    "    ax[1,2].set_title('Mask')\n",
    "    ax[2,0].set_title('Image')\n",
    "    ax[2,1].set_title('Prediction')\n",
    "    ax[2,2].set_title('Mask')\n",
    "    ax[0][0].axis(\"off\")\n",
    "    ax[1][0].axis(\"off\")\n",
    "    ax[2][0].axis(\"off\")\n",
    "    ax[0][1].axis(\"off\")\n",
    "    ax[1][1].axis(\"off\")\n",
    "    ax[2][1].axis(\"off\")\n",
    "    ax[0][2].axis(\"off\")\n",
    "    ax[1][2].axis(\"off\")\n",
    "    ax[2][2].axis(\"off\")\n",
    "    ax[0][0].imshow(img1)\n",
    "    ax[0][1].imshow(preds1)\n",
    "    ax[0][2].imshow(mask1)\n",
    "    ax[1][0].imshow(img2)\n",
    "    ax[1][1].imshow(preds2)\n",
    "    ax[1][2].imshow(mask2)\n",
    "    ax[2][0].imshow(img3)\n",
    "    ax[2][1].imshow(preds3)\n",
    "    ax[2][2].imshow(mask3)   \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
